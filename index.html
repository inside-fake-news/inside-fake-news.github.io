
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Inside Fake News</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
<style>
  .word-labels-container {
      font-family: Arial, sans-serif;
      margin: 10px 0;
      line-height: 1.6;
  }
  .summary-box {
      background-color: #f7e9e9;
      padding: 15px;
      border-radius: 5px;
      margin-bottom: 20px;
      border-left: 5px solid #e74c3c;
  }
  .item-box {
      margin-bottom: 20px;
      border: 1px solid #ddd;
      padding: 15px;
      border-radius: 5px;
      background-color: white;
      box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  }
  .item-header {
      background-color: #f0f0f0;
      padding: 10px;
      margin: -15px -15px 15px -15px;
      border-bottom: 1px solid #ddd;
      border-radius: 5px 5px 0 0;
      display: flex;
      justify-content: space-between;
  }
  .item-id {
      font-weight: bold;
  }
  .highlighted {
      color: red;
      font-weight: bold;
  }
  .text-content {
      padding: 10px;
      line-height: 1.8;
  }
  .section-title {
      font-weight: bold;
      margin-top: 15px;
      margin-bottom: 5px;
      color: #333;
      border-bottom: 1px solid #eee;
      padding-bottom: 5px;
  }
  .prompt-box {
      background-color: #f9f9f9;
      border-left: 3px solid #2c3e50;
      padding: 10px;
      margin-bottom: 15px;
      border-radius: 3px;
  }
  .marked-box {
      background-color: #fafafa;
      border-left: 3px solid #e74c3c;
      padding: 10px;
      border-radius: 3px;
  }
  .json-box {
      background-color: #f8f8f8;
      border-left: 3px solid #3498db;
      padding: 10px;
      border-radius: 3px;
      font-family: monospace;
      white-space: pre-wrap;
      overflow-x: auto;
  }
  details {
      margin-top: 10px;
      border: 1px solid #eaeaea;
      border-radius: 4px;
      padding: 0.5em;
  }
  summary {
      font-weight: bold;
      cursor: pointer;
      padding: 8px;
      background-color: #f3f3f3;
      border-radius: 3px;
  }
  details[open] summary {
      margin-bottom: 10px;
      border-bottom: 1px solid #eaeaea;
  }
</style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script>
    function showComingSoon(event) {
      event.preventDefault();
      
      // Create a modal/popup element
      const modal = document.createElement('div');
      modal.style.position = 'fixed';
      modal.style.left = '0';
      modal.style.top = '0';
      modal.style.width = '100%';
      modal.style.height = '100%';
      modal.style.backgroundColor = 'rgba(0,0,0,0.7)';
      modal.style.display = 'flex';
      modal.style.justifyContent = 'center';
      modal.style.alignItems = 'center';
      modal.style.zIndex = '1000';
      
      // Create message element
      const message = document.createElement('div');
      message.innerHTML = '<h2 style="color: white; font-size: 28px; font-weight: bold; text-align: center;">Coming Soon</h2>';
      message.style.padding = '30px 50px';
      message.style.backgroundColor = '#222';
      message.style.borderRadius = '10px';
      message.style.boxShadow = '0 5px 15px rgba(0,0,0,0.3)';
      
      // Add message to modal
      modal.appendChild(message);
      
      // Add modal to body
      document.body.appendChild(modal);
      
      // Close modal when clicked anywhere
      modal.addEventListener('click', function() {
        document.body.removeChild(modal);
      });
    }
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title"><i>Bridging Thoughts and Words:</i><br>Graph-Based Intent-Semantic Joint Learning<br>for Fake News Detection</h1>
            <p style="color: red; font-weight: bold; text-align: center; margin-top: 10px;">
              
            </p>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Anonymous Authors</a></span>
                <!-- <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Anonymous Author</a>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Anonymous Author</a>
                  </span> -->
                  </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark"
                        onclick="showComingSoon(event)">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper - Coming Soon</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://anonymous.4open.science/r/-Anonymous" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Anonymous)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark"
                  onclick="showComingSoon(event)">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv - Coming Soon</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="columns is-centered">
            <figure>
              <img src="static/images/motivation.png" style="width:75%" alt="Figure 0">
            </figure>
        </div>
      </div>
    </div> 
  </div>
  <br>    <br>
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Fake news detection is an important and challenging task for defending online information integrity. Existing state-of-the-art approaches typically extract news semantic clues, such as writing patterns that include emotional words, stylistic features, etc. However, detectors tuned solely to such semantic clues can easily fall into surface detection patterns, which can shift rapidly in dynamic environments, leading to limited performance in the evolving news landscape. To address this issue, this paper investigates a novel perspective by incorporating news intent into fake news detection, bridging intents and semantics together. The core insight is that by considering news intents, one can deeply understand the stable thoughts behind news deception, rather than the unstable surface patterns within words alone. To achieve this goal, we propose Graph-based Intent-Semantic Joint Modeling (Inside) for fake news detection, which models deception clues from both semantic and intent signals via graph-based joint learning. Specifically, Inside reformulates news semantic and intent signals into heterogeneous graph structures, enabling long-range context interaction through entity guidance and capturing both holistic and implementation-level intent via coarse-to-fine intent modeling. To achieve better alignment between semantics and intents, we further develop a dynamic pathway-based graph alignment strategy for effective message passing and aggregation across these signals by establishing a common space. Extensive experiments on four benchmark datasets demonstrate the superiority of the proposed Inside over other state-of-the-art methods.</p>
        </div>
      </div>
    </div>
  </div>
  <br>
  <br>
  <div class="container is-max-desktop">
    <!-- Method -->
    <div style="text-align:center">
      <h2 class="title is-3">Data & Environment Preparation</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          Our investigation requires a dataset that includes both real and fake news items written by humans or generated by LLMs with user-news interaction behavior records. As shown above, we construct a new dataset by 1) repurposing an existing human news dataset originally for fake news detection, 2) prompting two LLMs to generate news items according to the human news data and 3) performing a quality check. The new dataset will support the simulation of LLM-generated news involvement in news recommendation.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/data_construct.png" alt="Figure 1" style="width:60%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>

    <div style="text-align:center">
      <h2 class="title is-3">LLM-Based News Generation Modes</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          By referring to real-world cases and existing works, we compile a taxonomy of LLM-based news generation modes based on the degree of LLM involvement in generation. Specifically, we focus on the following four modes, which mimic the SAE's taxonomy of driving automation systems.
          <p>
              <b>L0: No Generator Automation (Human-Written).</b> The news piece is purely written by a human writer without any involvement of LLM generators.
          </p>
          <p>
              <b>L1: Human Assistance.</b> This is the lowest level of generation automation, where the generator provides simple enhancements to human-written news while preserving the original content and style. 
          </p>
          <p>
              <b>L2: Partial Generator Automation.</b> At this level, the generator has more freedom, modifying the style of human-written news while maintaining the core content. This results in news with a significantly altered style and some content changes.
          </p>
          <p>
              <b>L3: Conditional Generator Automation.</b> This level allows the generator even greater autonomy to create content based on provided news materials, resulting in significant changes in both content and style while retaining the news theme.
          </p>
      </p>

    </div>
    <br>
    <br>


    <div style="text-align:center">
      <h2 class="title is-3">How Does Generated News Affect the Neural News Recommendation System?</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          To investigate the impact of LLM-generated news on the news ecosystem, we conduct experiments at four distinct phases, based on the area intruded by LLM-generated news.
      </p>

    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/phases_intro.png" alt="Figure 1" style="width:60%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>


    <div style="text-align:center">
      <h2 class="title is-3">Phase 0: Generated News is Nowhere</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          To set a reference for subsequent LLM-involved experiments, we need to know the original neural news recommendation system purely trained on human-written news first. Specifically, we trained recommendation models using human-written news items and reported their ranking results of purely human-written candidate news items based on user history records of human-written news interactions.
          <br>
          From table, we observe that human-written real news consistently ranks higher than fake news. Across two recommendation models, real news outperforms fake news across all five ranking metrics. This suggests that news recommendation model tends to prioritize real news over fake news when only considering human data. This indicates that the original recommendation system, which only dealt with human-written news, somehow possesses a natural defense against fake news.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/phase_0_res.png" alt="Figure 1" style="width:60%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>



    <div style="text-align:center">
      <h2 class="title is-3">Phase 1: Generated News Enters Candidates</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          Initially, LLM-generated news just enters the candidate news pool, waiting for users' interactions. To simulate this stage where LLM-generated news is only in candidate lists, we trained recommendation models based on human-written news items and only include human-written news in the user history records, but the candidate list include both human-written and LLM-generated news items.
          From the table, we observe that:
          <br>
          <i><b>1) The ranking advantage of real news over fake news decreases, and in some cases, fake news even surpasses real news. </b></i> Comparing the RRA value with the baseline, we find that the introduction of LLM-generated fake news at all three faking levels reduces the ranking advantage of real news. At faking level L2, when the generator is Llama-3.1, real news even ranks lower than fake news. This indicates that the introduction of LLM-generated news significantly suppresses the ranking of real news, which is termed Truth Decay below.
          <br>
          <i><b>2)} LLM-generated news consistently ranks higher than human-written news.</b></i> This observation aligns with the finding in exsiting works, highlighting the advantage of LLM-generated text over human-written text.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/phase_1_res.png" alt="Figure 1" style="width:100%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>
    


    <div style="text-align:center">
      <h2 class="title is-3">Phase 2: Generated News Intrudes into User History</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          After a while, some LLM-generated news items successfully have interaction records, that is, generated news intrudes into user history.
          To simulate this phase, as what we did in Phase 1, we first trained news recommendation models based on human-written news items and then used the mixed candidate lists used in Phase 1 at the inference stage. However, the user history records now contain not only human-written news but also LLM-generated news.
          From the figure, we observe that:
          <br>
          <i><b>1) Generally, the RRA remains reduced compared to the baseline, indicating the truth decay phenomenon persists. </b></i> This trend holds across varying LLM ratios in user history, except for the scenario where the faking level is L3 and the generator is gpt-4o-mini.
          <br>
          <i><b>2) The overall trend of RRA varies across different faking levels.</b></i> At L1, the RRA remains stable across different LLM ratios. At L2, when the generator is gpt-4o-mini, the RRA remains stable; In contrast, with Llama-3.1, the RRA decreases further as the LLM ratio increases. At L3, the RRA slightly increases with gpt-4o-mini, whereas with Llama-3.1, the RRA decreases further as the LLM ratio increases.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/phase_2_res.png" alt="Figure 1" style="width:100%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>



    <div style="text-align:center">
      <h2 class="title is-3">Phase 3: Generated News Infiltrates Training Data</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          News recommendation models deployed in real-world applications update the weights through routine retraining using data collected in a recent interval. This gives a chance for LLM-generated news to <i><b>infiltrate</b></i> training data gradually and fundamentally affecting recommendation model training.
          To simulate this phase, we constructed the training data using both human-written news and LLM-generated news. Similar to what was done in Phase 2, we randomly replaced t% of human-written news items in training data with their corresponding LLM-generated news items. For inference, we employed mixed history records in Phase 2 with the same LLM ratio as in training data, and mixed candidate lists used in both Phases 1 and 2. 
          The figure illustrates the changes in Ratio@5 for real and fake news as the LLM ratio varies. We observe that:
          <br>
          <i><b>1) Overall, compared to the baseline, the presence of LLM-generated news in the training set consistently narrows the advantage of real news over fake news. </b></i>
          <br>
          <i><b>2) As the LLM ratio increases, this narrowing trend becomes more pronounced with higher faking levels.</b></i> At L1, the advantage of real news remains stable. At L2, the real news advantage significantly diminishes and may even reverse. At L3, the ratio@5 of fake news eventually surpasses that of real news as the LLM ratio increases, indicating that the system favors fake news over real news.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/phase_3_res.png" alt="Figure 1" style="width:100%; display:block; margin:0 auto;">
        </figure>
    </div>
    <br>
    <br>



    <div style="text-align:center">
      <h2 class="title is-3">Analysis: Why Truth Decay Occurs</h2>
    </div>
    <div class="content has-text-justified">
      <br>
      <p>
          We perform further analysis to provide a possible understanding of the underlying mechanisms by which truth decay occurs. Inspired by existing works, we focus on the backbone PLM used in news recommendation, as its intrinsic bias in language modeling would significantly affect content understanding and item scoring. Specifically, we consider the perplexity metric because it indicates the model's familiarity with a text: The lower the perplexity, the more familiar the model is with the given text, possibly leading to better modeling.
          The figure illustrates the perplexities (PPLs) of LLM-generated and human-written news. We observe that:
          <br>
          <i><b>1) In human-written news, real news exhibits lower perplexity than fake news, indicating an advantage of real over fake news, which is aligned with the real news advantages observed in phase 0. </b></i>
          <br>
          <i><b>2) Differently from human-written news, LLM-generated fake news has lower perplexity than the real, corresponding to the emergence of truth decay when LLM-generated news is included.</b></i>  Moreover, the perplexity is lowered as the faking level increases, which provides an explanation about why L3 brings more significant truth decay than L1 and L2---with fewer constraints, generated news at L3 has lower perplexity and is more preferred by the backbone of news recommenders.
      </p>
    </div>
    <div class="columns is-centered">
        <figure>
          <img src="static/short_cut/ppl_res.png" alt="Figure 1" style="width:100%; display:block; margin:0 auto;">
        </figure>
    </div>

    <!--/ Method -->
      </div>
    </div>






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming Soon</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
